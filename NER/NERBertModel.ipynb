{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle as pkl\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    CLS = [101]\n",
    "    SEP = [102]\n",
    "    VALUE_TOKEN = [0]\n",
    "    MAX_LEN = 128\n",
    "    TRAIN_BATCH_SIZE = 32\n",
    "    VAL_BATCH_SIZE = 8\n",
    "    EPOCHS = 3\n",
    "    TOKENIZER = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "  \n",
    "  def __init__(self, texts, tags):\n",
    "    \n",
    "    #Texts: [['Diana', 'is', 'a', 'girl], ['she', 'plays'. 'football']]\n",
    "    #tags: [[0, 1, 2, 5], [1, 3. 5]]\n",
    "    \n",
    "    self.texts = texts\n",
    "    self.tags = tags\n",
    "  \n",
    "  def __len__(self):\n",
    "    return len(self.texts)\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "    texts = self.texts[index]\n",
    "    tags = self.tags[index]\n",
    "  \n",
    "    #Токинезатор\n",
    "    ids = []\n",
    "    target_tag = []\n",
    "\n",
    "    for i, s in enumerate(texts):\n",
    "        inputs = Config.TOKENIZER.encode(s, add_special_tokens=False)\n",
    "     \n",
    "        input_len = len(inputs)\n",
    "        ids.extend(inputs)\n",
    "        target_tag.extend(input_len * [tags[i]])\n",
    "    \n",
    "    #To Add Special Tokens, subtract 2 from MAX_LEN\n",
    "    ids = ids[:Config.MAX_LEN - 2]\n",
    "    target_tag = target_tag[:Config.MAX_LEN - 2]\n",
    "\n",
    "    #Add Sepcial Tokens\n",
    "    ids = Config.CLS + ids + Config.SEP\n",
    "    target_tags = Config.VALUE_TOKEN + target_tag + Config.VALUE_TOKEN\n",
    "\n",
    "    mask = [1] * len(ids)\n",
    "    token_type_ids = [0] * len(ids)\n",
    "\n",
    "    #Add Padding if the input_len is small\n",
    "\n",
    "    padding_len = Config.MAX_LEN - len(ids)\n",
    "    ids = ids + ([0] * padding_len)\n",
    "    target_tags = target_tags + ([0] * padding_len)\n",
    "    mask = mask + ([0] * padding_len)\n",
    "    token_type_ids = token_type_ids + ([0] * padding_len)\n",
    "\n",
    "    return {\n",
    "        \"ids\" : torch.tensor(ids, dtype=torch.long),\n",
    "        \"mask\" : torch.tensor(mask, dtype=torch.long),\n",
    "        \"token_type_ids\" : torch.tensor(token_type_ids, dtype=torch.long),\n",
    "        \"target_tags\" : torch.tensor(target_tags, dtype=torch.long)\n",
    "      }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NERBertModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_tag):\n",
    "        super(NERBertModel, self).__init__()\n",
    "        self.num_tag = num_tag\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.bert_drop = nn.Dropout(0.3)\n",
    "        self.out_tag = nn.Linear(768, self.num_tag)\n",
    "        \n",
    "    def forward(self, ids, mask, token_type_ids, target_tags):\n",
    "        output, _ = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids, return_dict=False)\n",
    "        bert_out = self.bert_drop(output) \n",
    "        tag = self.out_tag(bert_out)\n",
    "    \n",
    "        #Calculate the loss\n",
    "        Critirion_Loss = nn.CrossEntropyLoss()\n",
    "        active_loss = mask.view(-1) == 1\n",
    "        active_logits = tag.view(-1, self.num_tag)\n",
    "        active_labels = torch.where(active_loss, target_tags.view(-1), torch.tensor(Critirion_Loss.ignore_index).type_as(target_tags))\n",
    "        loss = Critirion_Loss(active_logits, active_labels)\n",
    "        return tag, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"NER dataset.csv\", encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentence #</th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>Thousands</td>\n",
       "      <td>NNS</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>of</td>\n",
       "      <td>IN</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>demonstrators</td>\n",
       "      <td>NNS</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>have</td>\n",
       "      <td>VBP</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Sentence: 1</td>\n",
       "      <td>marched</td>\n",
       "      <td>VBN</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Sentence #           Word  POS  Tag\n",
       "0  Sentence: 1      Thousands  NNS   16\n",
       "1  Sentence: 1             of   IN   16\n",
       "2  Sentence: 1  demonstrators  NNS   16\n",
       "3  Sentence: 1           have  VBP   16\n",
       "4  Sentence: 1        marched  VBN   16"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Filling Missing Values and Label Encoding\n",
    "data[\"Sentence #\"] = data[\"Sentence #\"].fillna(method='ffill')\n",
    "le = LabelEncoder().fit(data['Tag'])\n",
    "data['Tag'] = le.transform(data['Tag'])\n",
    "pkl.dump(le, open('labelenc.pkl', 'wb'))\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>POS</th>\n",
       "      <th>Tag</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence #</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Sentence: 1</th>\n",
       "      <td>[Thousands, of, demonstrators, have, marched, ...</td>\n",
       "      <td>[NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...</td>\n",
       "      <td>[16, 16, 16, 16, 16, 16, 2, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 10</th>\n",
       "      <td>[Iranian, officials, say, they, expect, to, ge...</td>\n",
       "      <td>[JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...</td>\n",
       "      <td>[3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 100</th>\n",
       "      <td>[Helicopter, gunships, Saturday, pounded, mili...</td>\n",
       "      <td>[NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...</td>\n",
       "      <td>[16, 16, 7, 16, 16, 16, 16, 16, 2, 16, 16, 16,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 1000</th>\n",
       "      <td>[They, left, after, a, tense, hour-long, stand...</td>\n",
       "      <td>[PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]</td>\n",
       "      <td>[16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sentence: 10000</th>\n",
       "      <td>[U.N., relief, coordinator, Jan, Egeland, said...</td>\n",
       "      <td>[NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...</td>\n",
       "      <td>[2, 16, 16, 6, 14, 16, 7, 16, 2, 16, 3, 16, 3,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                              Word  \\\n",
       "Sentence #                                                           \n",
       "Sentence: 1      [Thousands, of, demonstrators, have, marched, ...   \n",
       "Sentence: 10     [Iranian, officials, say, they, expect, to, ge...   \n",
       "Sentence: 100    [Helicopter, gunships, Saturday, pounded, mili...   \n",
       "Sentence: 1000   [They, left, after, a, tense, hour-long, stand...   \n",
       "Sentence: 10000  [U.N., relief, coordinator, Jan, Egeland, said...   \n",
       "\n",
       "                                                               POS  \\\n",
       "Sentence #                                                           \n",
       "Sentence: 1      [NNS, IN, NNS, VBP, VBN, IN, NNP, TO, VB, DT, ...   \n",
       "Sentence: 10     [JJ, NNS, VBP, PRP, VBP, TO, VB, NN, TO, JJ, J...   \n",
       "Sentence: 100    [NN, NNS, NNP, VBD, JJ, NNS, IN, DT, NNP, JJ, ...   \n",
       "Sentence: 1000      [PRP, VBD, IN, DT, NN, JJ, NN, IN, NN, NNS, .]   \n",
       "Sentence: 10000  [NNP, NN, NN, NNP, NNP, VBD, NNP, ,, NNP, ,, J...   \n",
       "\n",
       "                                                               Tag  \n",
       "Sentence #                                                          \n",
       "Sentence: 1      [16, 16, 16, 16, 16, 16, 2, 16, 16, 16, 16, 16...  \n",
       "Sentence: 10     [3, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16...  \n",
       "Sentence: 100    [16, 16, 7, 16, 16, 16, 16, 16, 2, 16, 16, 16,...  \n",
       "Sentence: 1000        [16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16]  \n",
       "Sentence: 10000  [2, 16, 16, 6, 14, 16, 7, 16, 2, 16, 3, 16, 3,...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_gr = data.groupby(\"Sentence #\").agg({'Word': list, 'POS':list, 'Tag':list})\n",
    "data_gr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sent, val_sent, train_tag, val_tag = train_test_split(data_gr['Word'], data_gr['Tag'], test_size=0.01, random_state=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Dataset(texts = train_sent, tags = train_tag)\n",
    "val_dataset = Dataset(texts = val_sent, tags = val_tag)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=Config.TRAIN_BATCH_SIZE)\n",
    "val_data_loader = DataLoader(val_dataset, batch_size=Config.VAL_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tag = len(data.Tag.value_counts())\n",
    "model = NERBertModel(num_tag=num_tag)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_hyperparameters(model, ff):\n",
    "\n",
    "    # ff: full_finetuning\n",
    "    if ff:\n",
    "        param_optimizer = list(model.named_parameters())\n",
    "        no_decay = [\"bias\", \"gamma\", \"beta\"]\n",
    "        optimizer_grouped_parameters = [\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.01,\n",
    "            },\n",
    "            {\n",
    "                \"params\": [\n",
    "                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n",
    "                ],\n",
    "                \"weight_decay_rate\": 0.0,\n",
    "            },\n",
    "        ]\n",
    "    else:\n",
    "        param_optimizer = list(model.classifier.named_parameters())\n",
    "        optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
    "\n",
    "    return optimizer_grouped_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "FULL_FINETUNING = True\n",
    "optimizer_grouped_parameters = get_hyperparameters(model, FULL_FINETUNING)\n",
    "optimizer = torch.optim.AdamW(optimizer_grouped_parameters, lr=3e-5)\n",
    "num_train_steps = int(len(train_sent) / Config.TRAIN_BATCH_SIZE * Config.EPOCHS)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, \n",
    "    num_warmup_steps=0, \n",
    "    num_training_steps=num_train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction(test_sentence, model, le):\n",
    "    for i in list(string.punctuation):\n",
    "        test_sentence = test_sentence.replace(i, ' ' + i)\n",
    "    test_sentence = test_sentence.split()\n",
    "    print(test_sentence)\n",
    "    Token_inputs = Config.TOKENIZER.encode(test_sentence, add_special_tokens=False)\n",
    "    print(Token_inputs)\n",
    "    test_dataset =  Dataset(test_sentence, tags= [[1] * len(test_sentence)])\n",
    "    num_tag = len(le.classes_)\n",
    "   \n",
    "    with torch.no_grad():\n",
    "        data = test_dataset[0]\n",
    "        for i, j in data.items():\n",
    "            data[i] = j.to(device).unsqueeze(0)\n",
    "        tag, _ = model(**data)\n",
    "        print(le.inverse_transform(tag.argmax(2).cpu().numpy().reshape(-1))[1:len(Token_inputs)+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1484/1484 [11:47<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, phase: T, loss: 0.21848472558725074\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:03<00:00, 18.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, phase: V, loss: 0.1496036701525251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1484/1484 [11:48<00:00,  2.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, phase: T, loss: 0.14673179116933494\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:03<00:00, 18.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, phase: V, loss: 0.14289187093575795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1484/1484 [11:47<00:00,  2.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, phase: T, loss: 0.12597861566211818\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:03<00:00, 18.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, phase: V, loss: 0.14368095487977067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "phases = ['T', 'V']\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    for phase in phases:\n",
    "        loss_ = 0\n",
    "        if phase == 'T':\n",
    "            model.train()  # Установить модель в режим обучения\n",
    "            dataloader = train_data_loader\n",
    "        else:\n",
    "            model.eval()   #Установить модель в режим оценки\n",
    "            dataloader = val_data_loader\n",
    "        for data in tqdm(dataloader, total = len(dataloader)):\n",
    "            for i, j in data.items():\n",
    "                data[i] = j.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            with torch.set_grad_enabled(phase == 'T'):\n",
    "                _, loss = model(**data)\n",
    "                loss_ += loss.item()\n",
    "                if phase == 'T':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    scheduler.step()\n",
    "\n",
    "        print(f\"Epoch: {epoch + 1}, phase: {phase}, loss: {loss_ / len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Charles', 'I', 'was', 'born', 'in', 'Fife', 'on', '19', 'November', '1600', '.']\n",
      "[100, 100, 2001, 2141, 1999, 100, 2006, 2539, 100, 14883, 1012]\n",
      "['B-per' 'O' 'O' 'O' 'O' 'O' 'O' 'B-art' 'O' 'O' 'O']\n",
      "['She', 'is', 'playing', 'football', '.']\n",
      "[100, 2003, 2652, 2374, 1012]\n",
      "['B-per' 'I-per' 'I-org' 'B-art' 'B-per']\n"
     ]
    }
   ],
   "source": [
    "test_sentence = \"Charles I was born in Fife on 19 November 1600.\"\n",
    "prediction(test_sentence, model, le)\n",
    "test_sentence = \"She is playing football.\"\n",
    "prediction(test_sentence, model, le)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "301a17a29b57d3836b7901af1621afd6d2b1f2298b9c7949191147cf2fea93e3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
